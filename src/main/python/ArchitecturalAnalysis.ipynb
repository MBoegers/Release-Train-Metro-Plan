{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Release Train Metro Plan Analysis\n",
    "\n",
    "This notebook analyzes project relationships and generates visualization data for the release train metro plan.\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "The analysis uses three main data sources from OpenRewrite recipe runs:\n",
    "- **ProjectCoordinates.csv**: Maven/Gradle project identifiers (groupId, artifactId) \n",
    "- **DependenciesInUse.csv**: Dependencies between projects\n",
    "- **ParentRelationships.csv**: Parent POM and Gradle parent project relationships\n",
    "- **UnusedDependencies.csv**: Import patterns to identify potentially unused dependencies\n",
    "\n",
    "## Visualization Link Types\n",
    "\n",
    "The generated metro plan visualization supports multiple connection types:\n",
    "- **Dependency links** (blue solid): Normal dependencies between projects\n",
    "- **Parent links** (red solid): Parent POM or Gradle parent relationships  \n",
    "- **Unused links** (orange dashed): Potentially unused dependencies that should be reviewed\n",
    "\n",
    "Run the enhanced `ReleaseMetroPlan` recipe to generate all data files, then execute this notebook to create the visualization data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Configure the workspace path and recipe run ID, then load the CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Set\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure your workspace and recipe run\n",
    "# workspace = \"/Users/matt/workspaces/moderne-migration-workspace\"\n",
    "# recipe_run = \"20251217091702-WRNyJ\"\n",
    "\n",
    "workspace = \"/Users/matt/workspaces/app.moderne.io/Netflix_Spring_Apache\"\n",
    "recipe_run = \"20251217101017-MOHFU\"\n",
    "\n",
    "# Construct paths to data files\n",
    "datatables_path = os.path.join(workspace, \".moderne\", \"run\", recipe_run, \"datatables\")\n",
    "project_ids_path = os.path.join(datatables_path, \"dev.mboegie.rewrite.releasemetro.table.ProjectCoordinates.csv\")\n",
    "dependencies_path = os.path.join(datatables_path, \"org.openrewrite.maven.table.DependenciesInUse.csv\")\n",
    "parent_relationships_path = os.path.join(datatables_path, \"dev.mboegie.rewrite.releasemetro.table.ParentRelationships.csv\")\n",
    "\n",
    "# Load CSV files\n",
    "project_ids = pd.read_csv(project_ids_path)\n",
    "dependencies = pd.read_csv(dependencies_path)\n",
    "parent_relationships = pd.read_csv(parent_relationships_path)\n",
    "\n",
    "print(f\"Loaded {len(project_ids)} projects, {len(dependencies)} dependencies, and {len(parent_relationships)} parent relationships.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Structures\n",
    "\n",
    "Create classes to represent artifacts and repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Artifact:\n",
    "    group: Optional[str]\n",
    "    artifact: str\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Artifact):\n",
    "            return False\n",
    "        return self.group == other.group and self.artifact == other.artifact\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.group, self.artifact))\n",
    "\n",
    "@dataclass\n",
    "class ArtifactWithParent:\n",
    "    artifact: Artifact\n",
    "    parent: Optional[Artifact] = None\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, ArtifactWithParent):\n",
    "            return False\n",
    "        return self.artifact == other.artifact\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.artifact)\n",
    "\n",
    "@dataclass\n",
    "class Repository:\n",
    "    path: str\n",
    "    artifacts: Set[ArtifactWithParent]\n",
    "    dependencies: Set[Artifact]\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Repository):\n",
    "            return False\n",
    "        return self.path == other.path\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Repository Data\n",
    "\n",
    "Group projects by repository and create Repository objects with their artifacts and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = []\n",
    "\n",
    "# Filter for main/master branches\n",
    "project_ids_filtered = project_ids[\n",
    "    (project_ids['repositoryBranch'] == 'master') | \n",
    "    (project_ids['repositoryBranch'] == 'main')\n",
    "][['repositoryPath', 'groupId', 'artifactId']]\n",
    "\n",
    "dependencies_filtered = dependencies[\n",
    "    (dependencies['repositoryBranch'] == 'master') | \n",
    "    (dependencies['repositoryBranch'] == 'main')\n",
    "][['repositoryPath', 'groupId', 'artifactId']]\n",
    "\n",
    "# Group by repository path\n",
    "for repo_path, repo_projects in project_ids_filtered.groupby('repositoryPath'):\n",
    "    # Create artifacts for this repository\n",
    "    repo_artifacts = set()\n",
    "    for _, row in repo_projects.iterrows():\n",
    "        artifact = Artifact(row['groupId'] if pd.notna(row['groupId']) else None, row['artifactId'])\n",
    "        repo_artifacts.add(ArtifactWithParent(artifact))\n",
    "    \n",
    "    # Get dependencies for this repository\n",
    "    repo_deps = dependencies_filtered[dependencies_filtered['repositoryPath'] == repo_path]\n",
    "    repo_dependencies = set()\n",
    "    for _, row in repo_deps.iterrows():\n",
    "        artifact = Artifact(row['groupId'] if pd.notna(row['groupId']) else None, row['artifactId'])\n",
    "        repo_dependencies.add(artifact)\n",
    "    \n",
    "    repos.append(Repository(repo_path, repo_artifacts, repo_dependencies))\n",
    "\n",
    "print(f\"Created {len(repos)} repositories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Parent Relationships\n",
    "\n",
    "Add parent relationship information to artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter parent relationships for main/master branches\n",
    "parent_relationships_filtered = parent_relationships[\n",
    "    (parent_relationships['repositoryBranch'] == 'master') | \n",
    "    (parent_relationships['repositoryBranch'] == 'main')\n",
    "]\n",
    "\n",
    "if len(parent_relationships_filtered) > 0:\n",
    "    for _, row in parent_relationships_filtered.iterrows():\n",
    "        repo_path = row['repositoryPath']\n",
    "        child_artifact_id = row['childArtifactId']\n",
    "        parent_group_id = row['parentGroupId'] if pd.notna(row['parentGroupId']) else None\n",
    "        parent_artifact_id = row['parentArtifactId']\n",
    "        \n",
    "        # Find the repository\n",
    "        repo = next((r for r in repos if r.path == repo_path), None)\n",
    "        if repo:\n",
    "            # Find the artifact and set its parent\n",
    "            for artifact_with_parent in repo.artifacts:\n",
    "                if artifact_with_parent.artifact.artifact == child_artifact_id:\n",
    "                    artifact_with_parent.parent = Artifact(parent_group_id, parent_artifact_id)\n",
    "                    break\n",
    "else:\n",
    "    print(\"No parent relationships found - skipping parent relationship processing\")\n",
    "\n",
    "total_artifacts = sum(len(r.artifacts) for r in repos)\n",
    "total_dependencies = sum(len(r.dependencies) for r in repos)\n",
    "total_parents = sum(1 for r in repos for a in r.artifacts if a.parent is not None)\n",
    "\n",
    "print(f\"Derived {len(repos)} repositories from the data, containing {total_artifacts} artifacts, {total_dependencies} dependencies, and {total_parents} parent relationships.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Graph Edges\n",
    "\n",
    "Create connections between repositories based on dependencies and parent relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class LinkType(Enum):\n",
    "    PARENT = \"parent\"\n",
    "    DEPENDENCY = \"dependency\"\n",
    "    UNUSED = \"unused\"\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Link:\n",
    "    src: str\n",
    "    dist: str\n",
    "    type: LinkType\n",
    "    \n",
    "    def as_d3(self) -> str:\n",
    "        return f'{{ source: \"{self.src}\", target: \"{self.dist}\", type: \"{self.type.value}\" }}'\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Node:\n",
    "    id: str\n",
    "    \n",
    "    def as_d3(self) -> str:\n",
    "        return f'{{ id: \"{self.id}\" }}'\n",
    "\n",
    "edges = set()\n",
    "\n",
    "for repo in repos:\n",
    "    # Add parent relationships: if artifact A has parent B, create link from A's repo to B's repo\n",
    "    for artifact_with_parent in repo.artifacts:\n",
    "        if artifact_with_parent.parent is not None:\n",
    "            # Find repository that contains the parent artifact\n",
    "            parent_repo = next(\n",
    "                (r for r in repos \n",
    "                 if any(a.artifact == artifact_with_parent.parent for a in r.artifacts)),\n",
    "                None\n",
    "            )\n",
    "            if parent_repo is not None and parent_repo.path != repo.path:\n",
    "                edges.add(Link(repo.path, parent_repo.path, LinkType.PARENT))\n",
    "    \n",
    "    # Add dependency relationships: if repo uses dependency D, create link from repo to D's repo\n",
    "    for dep in repo.dependencies:\n",
    "        dep_repo = next(\n",
    "            (r for r in repos if any(a.artifact == dep for a in r.artifacts)),\n",
    "            None\n",
    "        )\n",
    "        if dep_repo is not None and dep_repo.path != repo.path:\n",
    "            edges.add(Link(repo.path, dep_repo.path, LinkType.DEPENDENCY))\n",
    "\n",
    "print(f\"Generated {len(edges)} edges from dependencies and parent relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Unused Dependencies\n",
    "\n",
    "Analyze potentially unused dependencies if the data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    unused_deps_path = os.path.join(datatables_path, \"dev.mboegie.rewrite.releasemetro.table.UnusedDependencies.csv\")\n",
    "    unused_deps = pd.read_csv(unused_deps_path)\n",
    "    \n",
    "    # Group unused dependencies by repository and find potential unused links\n",
    "    unused_by_repo = {}\n",
    "    \n",
    "    for repo_path, group in unused_deps.groupby('repositoryPath'):\n",
    "        # Filter for rows where reasonSuspected contains \"Import found\"\n",
    "        import_found = group[group['reasonSuspected'].str.contains('Import found', na=False)]\n",
    "        \n",
    "        # Group by dependencyGroupId and filter for those with very few imports\n",
    "        suspicious_deps = []\n",
    "        for dep_group_id, dep_group in import_found.groupby('dependencyGroupId'):\n",
    "            if len(dep_group) < 2:  # Dependencies with very few imports\n",
    "                suspicious_deps.append(dep_group_id)\n",
    "        \n",
    "        if suspicious_deps:\n",
    "            unused_by_repo[repo_path] = suspicious_deps\n",
    "    \n",
    "    # Create unused dependency links for dependencies with minimal usage\n",
    "    for repo_path, suspicious_deps in unused_by_repo.items():\n",
    "        for dep_group_id in suspicious_deps:\n",
    "            # Find repository that has an artifact with this groupId\n",
    "            target_repo = next(\n",
    "                (r for r in repos if any(a.artifact.group == dep_group_id for a in r.artifacts)),\n",
    "                None\n",
    "            )\n",
    "            \n",
    "            if target_repo is not None and target_repo.path != repo_path:\n",
    "                # Only add if there's already a dependency link (to avoid false positives)\n",
    "                existing_dep = next(\n",
    "                    (e for e in edges \n",
    "                     if e.src == repo_path and e.dist == target_repo.path and e.type == LinkType.DEPENDENCY),\n",
    "                    None\n",
    "                )\n",
    "                if existing_dep is not None:\n",
    "                    edges.add(Link(repo_path, target_repo.path, LinkType.UNUSED))\n",
    "                    print(f\"Added unused dependency link: {repo_path} -> {target_repo.path} ({dep_group_id})\")\n",
    "    \n",
    "    print(f\"Processed {len(unused_by_repo)} repositories for unused dependency analysis\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"UnusedDependencies.csv not available - skipping unused dependency link generation\")\n",
    "    print(\"Run FindPotentiallyUnusedDependencies recipe to enable unused dependency analysis\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing unused dependencies: {e}\")\n",
    "    print(\"Skipping unused dependency link generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Nodes and Summary\n",
    "\n",
    "Extract nodes from edges and display summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate nodes from edges\n",
    "node_ids = set()\n",
    "for edge in edges:\n",
    "    node_ids.add(edge.src)\n",
    "    node_ids.add(edge.dist)\n",
    "\n",
    "nodes = [Node(node_id) for node_id in sorted(node_ids)]\n",
    "\n",
    "# Count edges by type\n",
    "dependency_count = sum(1 for e in edges if e.type == LinkType.DEPENDENCY)\n",
    "parent_count = sum(1 for e in edges if e.type == LinkType.PARENT)\n",
    "unused_count = sum(1 for e in edges if e.type == LinkType.UNUSED)\n",
    "\n",
    "print(f\"\\nGenerated {len(edges)} total connections:\")\n",
    "print(f\"- {dependency_count} dependency links\")\n",
    "print(f\"- {parent_count} parent links\")\n",
    "print(f\"- {unused_count} unused dependency links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Output to JavaScript File\n",
    "\n",
    "Generate the JavaScript file that will be used by the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the project root (assuming notebook is in src/main/python/)\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent.parent.parent\n",
    "output_path = project_root / \"src\" / \"main\" / \"static\" / \"data\" / \"connections.js\"\n",
    "\n",
    "# Alternative: use absolute path if needed\n",
    "# output_path = Path(\"/Users/matt/projects/mboegers/Release-Train-Metro-Plan/src/main/static/data/connections.js\")\n",
    "\n",
    "# Generate JavaScript code\n",
    "nodes_js = \"const nodes = [\\n\\t\" + \",\\n\\t\".join(node.as_d3() for node in nodes) + \"\\n];\"\n",
    "links_js = \"const links = [\\n\\t\" + \",\\n\\t\".join(edge.as_d3() for edge in sorted(edges, key=lambda e: (e.src, e.dist))) + \"\\n];\"\n",
    "\n",
    "output_content = nodes_js + \"\\n\" + links_js\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Write to file\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write(output_content)\n",
    "\n",
    "print(f\"\\nWrote visualization data to: {output_path}\")\n",
    "print(f\"Open {project_root / 'src' / 'main' / 'static' / 'metro-plan.html'} in a browser to view the metro plan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Enhanced Visualization\n",
    "\n",
    "After running this notebook, open `metro-plan.html` in a browser.\n",
    "\n",
    "### Visual Legend:\n",
    "- **Blue solid lines + arrows**: Regular dependency relationships  \n",
    "- **Red solid lines + arrows**: Parent POM/Gradle relationships\n",
    "- **Orange dashed lines + arrows**: Potentially unused dependencies (review candidates)\n",
    "\n",
    "### Interpreting Unused Dependencies:\n",
    "Orange dashed lines indicate dependencies that are declared in build files but have minimal import usage in the source code. These represent potential cleanup opportunities:\n",
    "\n",
    "1. **Review the dependency**: Check if it's actually needed\n",
    "2. **Consider removal**: If unused, removing it can simplify the release train\n",
    "3. **Update build files**: Remove unnecessary dependencies to reduce coupling\n",
    "\n",
    "The dashed visualization makes it easy to spot problematic dependencies that may be complicating your release coordination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Unused Dependencies (Optional)\n",
    "\n",
    "Additional analysis of unused dependencies patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    unused_deps_path = os.path.join(datatables_path, \"dev.mboegie.rewrite.releasemetro.table.UnusedDependencies.csv\")\n",
    "    unused_deps = pd.read_csv(unused_deps_path)\n",
    "    \n",
    "    print(\"=== Unused Dependencies Analysis ===\")\n",
    "    print(f\"Found {len(unused_deps)} import usage records\")\n",
    "    \n",
    "    # Group by dependency to see usage patterns\n",
    "    dependency_usage = unused_deps.groupby('dependencyGroupId').agg(\n",
    "        usageCount=('dependencyGroupId', 'count'),\n",
    "        artifactId=('dependencyArtifactId', 'first')\n",
    "    ).sort_values('usageCount', ascending=False)\n",
    "    \n",
    "    print(\"\\nMost imported dependency groups:\")\n",
    "    for idx, (group_id, row) in enumerate(dependency_usage.head(10).iterrows()):\n",
    "        print(f\"{group_id}: {row['usageCount']} imports\")\n",
    "    \n",
    "    # Identify potentially problematic dependencies\n",
    "    import_found = unused_deps[unused_deps['reasonSuspected'].str.contains('Import found', na=False)]\n",
    "    suspicious_deps = import_found.groupby('dependencyGroupId').size()\n",
    "    suspicious_deps = suspicious_deps[suspicious_deps < 3]  # Dependencies with very few imports\n",
    "    \n",
    "    print(\"\\nDependencies with minimal usage (< 3 imports):\")\n",
    "    for group_id, count in suspicious_deps.items():\n",
    "        print(f\"{group_id}: {count} imports\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"UnusedDependencies.csv not found - run FindPotentiallyUnusedDependencies recipe first\")\n",
    "    print(\"This analysis shows import patterns that can help identify unused dependencies\")\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing unused dependencies: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moderne-vis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
